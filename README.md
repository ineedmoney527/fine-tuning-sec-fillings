# Financial Extraction Engine

Fine-tune Qwen3-8B-Instruct to extract structured JSON metrics from SEC 10-K filings.

## Tech Stack

- **Data pipeline**: Python, `sec-api`, `docling` (HTMLâ†’Markdown)
- **Training**: `unsloth` (fast QLoRA), Hugging Face `trl`
- **Inference**: `outlines` or `pydantic` for structured generation
- **Prompt format**: ChatML (`<|im_start|>...`)
- **Code standards**: Modular, type-hinted, `loguru` for logging

## Project Structure

```
financial-extraction-engine/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw 10-K HTML files (Item 8)
â”‚   â”œâ”€â”€ processed/        # Clean Markdown files
â”‚   â””â”€â”€ train.jsonl       # Training dataset (ChatML format)
â”œâ”€â”€ src/                  # Source code modules (coming soon)
â”œâ”€â”€ logs/                 # Application logs
â”œâ”€â”€ download_filings.py   # SEC 10-K downloader
â”œâ”€â”€ process_data.py       # HTML to Markdown converter
â”œâ”€â”€ generate_dataset.py   # Dataset generator (DeepSeek-V3 teacher)
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ DATASET_USAGE.md      # Dataset generation guide
â””â”€â”€ README.md             # This file
```

## Setup

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

**Note**: `docling` is the primary HTMLâ†’Markdown converter. If installation fails (heavy dependencies), the script will automatically fall back to `markdownify`.

### 2. Set API Keys

**SEC API** (for downloading filings):

```bash
export SEC_API_KEY="your-sec-api-key"
```

Get your key from [sec-api.io](https://sec-api.io/)

**DeepSeek API** (for dataset generation):

```bash
export DEEPSEEK_API_KEY="your-deepseek-api-key"
```

Get your key from [platform.deepseek.com](https://platform.deepseek.com/)

## Usage

### 1. Download 10-K Item 8 Filings

Download Item 8 (Financial Statements) from 10-K filings for AAPL, MSFT, TSLA, NVDA, GOOGL (2023-2025):

```bash
python -m src.download
```

**Output**: HTML files saved to `data/raw/{ticker}_{year}.html`

**Features**:
- âœ… Automatic rate limiting (0.3s between API calls)
- âœ… Comprehensive error handling and retry logic
- âœ… Detailed logging to both console and `logs/` directory
- âœ… Progress tracking and download statistics

### 2. Convert HTML to Clean Markdown

Process raw HTML files into clean Markdown with table preservation:

```bash
python -m src.process_data
```

**Output**: Markdown files saved to `data/processed/{ticker}_{year}.md`

**Features**:
- âœ… **Table preservation**: Converts HTML tables to Markdown pipe syntax
- âœ… **Noise removal**: Removes lines with <5 words (configurable)
- âœ… **Dual converter support**: Uses `docling` (primary) or `markdownify` (fallback)
- âœ… **Smart cleaning**: Preserves headers, tables, and meaningful content

### 3. Generate Fine-Tuning Dataset

Use DeepSeek-V3 as a teacher model to label the data:

```bash
# Set DeepSeek API key
export DEEPSEEK_API_KEY="your-deepseek-api-key"

# Generate dataset
python -m src.generate_dataset
```

**Output**: ChatML-formatted JSONL file saved to `data/train.jsonl`

**Features**:
- âœ… **Teacher-student paradigm**: Uses DeepSeek-V3 to label data for Qwen3
- âœ… **Number normalization**: Converts millions/billions to full values
- âœ… **ChatML format**: Compatible with Qwen3 fine-tuning
- âœ… **Structured extraction**: Extracts 6 key financial metrics
- âœ… **Rate limiting**: Respects API limits with delays

**Extracted metrics**:
- Revenue
- Operating Income  
- Net Income
- Total Assets
- Cash and Cash Equivalents
- Earnings Per Share (Diluted)

## Configuration

### src.download and src.process_data
- **Tickers/Years**: Modify variables in `src/download.py`
- **Rate limit**: Adjust `RATE_LIMIT_DELAY` in `src/download.py`
- **Min words**: Adjust `MIN_WORDS_PER_LINE` in `src/process_data.py`

### src.generate_dataset
- **Teacher model**: Change `MODEL_NAME` (default: `deepseek-chat`)
- **Target metrics**: Modify `TARGET_METRICS` list
- **System prompt**: Customize `SYSTEM_PROMPT` for different extraction rules

## Fine-Tuning with QLoRA

### Prerequisites from requirements.txt
Install training dependencies (requires GPU with ~24GB VRAM):

```bash
pip install -r requirements.txt
```

### Training

Run the QLoRA fine-tuning script:

```bash
# Full training run (requires GPU)
python -m src.train --output_dir outputs/qwen3-8b-financial-lora

# Dry run with 2 steps (for testing)
python -m src.train --max_steps 2 --output_dir outputs/test_run
```

**Training Configuration:**
- Model: `unsloth/Qwen3-8B-Instruct-unsloth-bnb-4bit`
- LoRA rank: 16, alpha: 32
- Learning rate: 2e-4
- Epochs: 3

### Inference with Pydantic Validation

The inference module includes automatic retry logic for malformed JSON outputs:

```python
from src.inference import FinancialExtractor

# Load fine-tuned model
extractor = FinancialExtractor("outputs/qwen3-8b-financial-lora")

# Extract metrics (with automatic retry on validation errors)
result = extractor.extract(markdown_content)

# Access validated metrics
print(result.metrics.revenue)       # int or None
print(result.metrics.net_income)    # int or None
print(result.metrics.diluted_eps)   # float or None
```

**Features:**
- âœ… Pydantic validation for structured output
- âœ… Automatic retry with error feedback (max 3 attempts)
- âœ… Type coercion (strings â†’ ints/floats)
- âœ… Null handling for missing data

## Project Status

1. âœ… **Data collection** with `sec-api`
2. âœ… **HTMLâ†’Markdown conversion** with `docling`
3. âœ… **Dataset generation** with DeepSeek-V3 teacher
4. âœ… **Fine-tuning pipeline** with `unsloth` and QLoRA
5. âœ… **Structured inference** with Pydantic validation + retry
6. ðŸ”² **Evaluation and deployment**

## License

MIT
