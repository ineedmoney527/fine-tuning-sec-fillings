{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Qwen3-8B for Financial Extraction (Colab)\n",
    "\n",
    "This notebook fine-tunes Qwen3-8B-Instruct using QLoRA and Unsloth for structured financial entity extraction from SEC 10-K reports.\n",
    "\n",
    "**Requirements**: T4 GPU (free tier) or A100 (Colab Pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes\n",
    "!pip install datasets pydantic loguru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Training Data\n",
    "\n",
    "Upload your `train.jsonl` file or clone your repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Clone your GitHub repo\n",
    "!git clone https://github.com/ineedmoney527/fine-tuning-sec-fillings.git\n",
    "%cd fine-tuning-sec-fillings\n",
    "\n",
    "# Option B: Upload manually (uncomment below)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"unsloth/Qwen3-8B-Instruct-unsloth-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 8192\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,              # LoRA rank\n",
    "    lora_alpha=32,     # Scaling factor\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Set up chat template\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"LoRA adapters configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load JSONL data\n",
    "DATA_PATH = \"data/train.jsonl\"  # Adjust path if needed\n",
    "\n",
    "data = []\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "dataset = Dataset.from_list([{\"messages\": ex[\"messages\"]} for ex in data])\n",
    "print(f\"Loaded {len(dataset)} training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "def formatting_func(examples):\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# Training configuration\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./outputs\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()\n",
    "print(f\"Training complete! Final loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "model.save_pretrained(\"./outputs/qwen3-8b-financial-lora\")\n",
    "tokenizer.save_pretrained(\"./outputs/qwen3-8b-financial-lora\")\n",
    "print(\"Model saved to ./outputs/qwen3-8b-financial-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download the adapter\n",
    "!zip -r qwen3-financial-lora.zip ./outputs/qwen3-8b-financial-lora\n",
    "\n",
    "from google.colab import files\n",
    "files.download('qwen3-financial-lora.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Extract financial metrics as JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Revenue: $100 million. Net Income: $10 million. Total Assets: $500 million.\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "print(\"Model output:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
