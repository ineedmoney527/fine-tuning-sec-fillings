{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Qwen3-8B for Financial Extraction (Colab)\n",
    "\n",
    "Fine-tunes Qwen3-8B-Instruct using QLoRA for SEC 10-K extraction.\n",
    "\n",
    "**Features**: Custom loss, `/no_think` mode, **saves merged model for MLX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install unsloth\n",
    "%pip install --no-deps trl peft accelerate bitsandbytes\n",
    "%pip install datasets pydantic loguru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ineedmoney527/fine-tuning-sec-fillings.git\n",
    "%cd fine-tuning-sec-fillings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "MODEL_NAME = \"unsloth/Qwen3\"\n",
    "MAX_SEQ_LENGTH = 8192\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "print(f\"Loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model, r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\", use_gradient_checkpointing=\"unsloth\", random_state=42,\n",
    ")\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"LoRA configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "data = [json.loads(line) for line in open(\"data/train.jsonl\") if line.strip()]\n",
    "\n",
    "formatted_data = []\n",
    "for ex in data:\n",
    "    messages = [{'role': m['role'], 'content': m['content'] + ' /no_think' if m['role']=='user' else m['content']} for m in ex['messages']]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    formatted_data.append({\"text\": text})\n",
    "\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "print(f\"Loaded {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# CRITICAL: Enable logits for custom loss (Unsloth disables by default since Nov 2024)\n",
    "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n",
    "\n",
    "FINANCIAL_KEYS = {\"revenue\", \"net_income\", \"operating_income\", \"total_assets\", \"cash_and_equivalents\", \"diluted_eps\", \"value\", \"unit\"}\n",
    "WEIGHTS = {\"json_key\": 2.0, \"number\": 1.5, \"json_structure\": 1.2, \"default\": 1.0}\n",
    "\n",
    "class CustomFinancialTrainer(SFTTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        tok = self.processing_class\n",
    "        self.key_tokens = set(sum([tok.encode(f'\"{k}\"', add_special_tokens=False) + tok.encode(k, add_special_tokens=False) for k in FINANCIAL_KEYS], []))\n",
    "        self.struct_tokens = set(sum([tok.encode(c, add_special_tokens=False) for c in '{}[]:\",'], []))\n",
    "        self.digit_tokens = set(sum([tok.encode(d, add_special_tokens=False) for d in '0123456789.-'], []))\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\", inputs[\"input_ids\"].clone())\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        \n",
    "        weights = torch.ones_like(shift_labels, dtype=torch.float32)\n",
    "        for b in range(shift_labels.shape[0]):\n",
    "            for i in range(shift_labels.shape[1]):\n",
    "                lid = shift_labels[b,i].item()\n",
    "                if lid == -100: weights[b,i] = 0.0\n",
    "                elif lid in self.key_tokens: weights[b,i] = WEIGHTS[\"json_key\"]\n",
    "                elif lid in self.struct_tokens: weights[b,i] = WEIGHTS[\"json_structure\"]\n",
    "                elif lid in self.digit_tokens: weights[b,i] = WEIGHTS[\"number\"]\n",
    "        weights = weights.to(logits.device)\n",
    "        \n",
    "        loss = torch.nn.CrossEntropyLoss(reduction='none')(logits.view(-1, logits.size(-1)), shift_labels.view(-1))\n",
    "        mask = weights.view(-1) > 0\n",
    "        loss = (loss * weights.view(-1))[mask].sum() / weights.view(-1)[mask].sum() if mask.sum() > 0 else loss.sum()\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"CustomFinancialTrainer ready (logits enabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./outputs\", per_device_train_batch_size=2, gradient_accumulation_steps=4,\n",
    "    warmup_steps=5, num_train_epochs=3, learning_rate=2e-4, fp16=True,\n",
    "    logging_steps=1, save_strategy=\"epoch\", optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01, lr_scheduler_type=\"linear\", seed=42,\n",
    "    max_seq_length=MAX_SEQ_LENGTH, packing=False, dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer = CustomFinancialTrainer(model=model, tokenizer=tokenizer, train_dataset=dataset, args=training_args)\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
